# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.metrics import (accuracy_score, precision_score, recall_score, 
                           f1_score, roc_auc_score, confusion_matrix, 
                           classification_report, roc_curve, precision_recall_curve)
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.impute import SimpleImputer
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.pipeline import Pipeline
from sklearn.compose import ColumnTransformer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline
import shap
import joblib
import warnings
warnings.filterwarnings('ignore')

# Set visualization style
sns.set_style('whitegrid')
plt.rcParams['figure.figsize'] = (10, 6)

# Load the dataset
df = pd.read_csv("C:/Users/arpit/Documents/credit_risk.csv")

# Data dictionary
data_dict = {
    'Id': 'Unique identifier for each loan application',
    'Age': 'Age of the applicant',
    'Income': 'Annual income of the applicant',
    'Home': 'Home ownership status (RENT, OWN, MORTGAGE, OTHER)',
    'Emp_length': 'Employment length in months',
    'Intent': 'Loan intent (PERSONAL, EDUCATION, MEDICAL, VENTURE, etc.)',
    'Amount': 'Loan amount requested',
    'Rate': 'Interest rate on the loan',
    'Status': 'Loan status (binary)',
    'Percent_income': 'Loan amount as percentage of income',
    'Default': 'Target variable - whether loan defaulted (Y/N)',
    'Cred_length': 'Length of credit history in years'
}

# Basic validation
print(f"Dataset shape: {df.shape}")
print("\nData types:")
print(df.dtypes)
print("\nMissing values:")
print(df.isnull().sum())
print("\nDuplicate rows:", df.duplicated().sum())

# Check the first few rows of the dataset to inspect the 'Default' column
print(df.head())

# Check for missing values across the entire dataset
print(df.isna().sum())


# Calculate percentage of missing values
missing_percent = df.isnull().sum() / len(df) * 100
missing_percent = missing_percent[missing_percent > 0].sort_values(ascending=False)
print("Missing values percentage:")
print(missing_percent)

# Visualize missing values
plt.figure(figsize=(10, 5))
sns.barplot(x=missing_percent.index, y=missing_percent.values)
plt.title('Percentage of Missing Values by Feature')
plt.ylabel('Percentage')
plt.xticks(rotation=45)
plt.show()

# Imputation strategy
# For Rate - median by loan intent
df['Rate'] = df.groupby('Intent')['Rate'].transform(lambda x: x.fillna(x.median()))

# For Emp_length - median by income quartile
df['Income_quartile'] = pd.qcut(df['Income'], q=4, labels=False)
df['Emp_length'] = df.groupby('Income_quartile')['Emp_length'].transform(lambda x: x.fillna(x.median()))
df.drop('Income_quartile', axis=1, inplace=True)

# Check for outliers in numerical features
num_cols = ['Age', 'Income', 'Emp_length', 'Amount', 'Rate', 'Percent_income', 'Cred_length']

plt.figure(figsize=(15, 10))
for i, col in enumerate(num_cols, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=df[col])
    plt.title(f'Boxplot of {col}')
plt.tight_layout()
plt.show()

# Handle extreme values
# Age - cap at 100 (there's an entry with age 144 which is likely an error)
df['Age'] = df['Age'].apply(lambda x: 100 if x > 100 else x)

# Income - cap at 99th percentile
income_99 = df['Income'].quantile(0.99)
df['Income'] = df['Income'].apply(lambda x: income_99 if x > income_99 else x)

# Emp_length - cap at 120 months (10 years)
df['Emp_length'] = df['Emp_length'].apply(lambda x: 120 if x > 120 else x)

# Check distributions after handling outliers
plt.figure(figsize=(15, 10))
for i, col in enumerate(num_cols, 1):
    plt.subplot(3, 3, i)
    sns.histplot(df[col], kde=True)
    plt.title(f'Distribution of {col}')
plt.tight_layout()
plt.show()

# Convert 'Default' column to numeric (1 for 'Y', 0 for 'N')
df['Default'] = df['Default'].map({'Y': 1, 'N': 0})

# Target variable distribution
plt.figure(figsize=(8, 5))
sns.countplot(x='Default', data=df)
plt.title('Distribution of Default (Target Variable)')
plt.show()

# Default rate by categorical features
cat_cols = ['Home', 'Intent', 'Status']

plt.figure(figsize=(15, 5))
for i, col in enumerate(cat_cols, 1):
    plt.subplot(1, 3, i)
    df.groupby(col)['Default'].value_counts(normalize=True).unstack().plot(kind='bar', stacked=True)
    plt.title(f'Default Rate by {col}')
    plt.ylabel('Proportion')
plt.tight_layout()
plt.show()

# Correlation heatmap
plt.figure(figsize=(10, 8))
corr = df[num_cols + ['Default']].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', center=0)
plt.title('Correlation Heatmap')
plt.show()

# Default rate by numerical features (binned)
plt.figure(figsize=(15, 10))
for i, col in enumerate(num_cols, 1):
    plt.subplot(3, 3, i)
    df['Default_num'] = df['Default']  # Already mapped to 1/0 earlier
    sns.barplot(x=pd.qcut(df[col], q=5, duplicates='drop'), y='Default_num', data=df)
    plt.title(f'Default Rate by {col} Quintiles')
    plt.xticks(rotation=45)
plt.tight_layout()
plt.show()

# Cleanup
df.drop('Default_num', axis=1, inplace=True)


# Import required libraries (in case not already done)
import pandas as pd
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.compose import ColumnTransformer

# ✅ Step 1: Check unique values in 'Default' before mapping
print("Original unique values in 'Default' column:")
print(df['Default'].unique())

# Convert 'Default' column to a more flexible binary representation
df['Default'] = df['Default'].astype(str).str.strip().str.lower().map({
    'y': 1, 'yes': 1, '1': 1,
    'n': 0, 'no': 0, '0': 0
})

# Check for any unmapped values (which will become NaN)
if df['Default'].isna().any():
    print("⚠️ Warning: Some values in 'Default' were not mapped correctly!")
    print(df['Default'].value_counts(dropna=False))

# Optional: Fill NaN values in 'Default' with 0 (indicating no default)
df['Default'].fillna(0, inplace=True)

# ✅ Step 2: Separate features and target
X = df.drop('Default', axis=1)
y = df['Default']

# ✅ Step 3: Create new features before defining feature lists
def create_features(df):
    df = df.copy()

    # Debt-to-income ratio
    df['debt_to_income'] = df['Amount'] / df['Income']
    
    # Age buckets
    df['age_bucket'] = pd.cut(df['Age'], 
                              bins=[0, 30, 50, 100],
                              labels=['Young', 'Middle-aged', 'Senior'],
                              include_lowest=True)
    
    # Loan-to-income ratio
    df['loan_to_income'] = df['Amount'] / df['Income']
    
    # Interaction term: Rate * Percent_income
    df['rate_income_interaction'] = df['Rate'] * df['Percent_income']
    
    return df

# ✅ Step 4: Apply feature engineering
X = create_features(X)

# ✅ Step 5: Define categorical and numerical features (updated)
cat_features = ['Home', 'Intent', 'Status', 'age_bucket']
num_features = ['Age', 'Income', 'Emp_length', 'Amount', 'Rate', 
                'Percent_income', 'Cred_length', 'debt_to_income', 
                'loan_to_income', 'rate_income_interaction']

# ✅ Step 6: Preprocessing pipeline
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, num_features),
        ('cat', categorical_transformer, cat_features)
    ]
)

# Verify the unique values in the 'Default' column and check class distribution
print(df['Default'].unique())
print(df['Default'].value_counts(dropna=False))

# Create new features
def create_features(df):
    df = df.copy()
    # Debt-to-income ratio
    df['debt_to_income'] = df['Amount'] / df['Income']
    
    # Age buckets
    df['age_bucket'] = pd.cut(df['Age'], 
                             bins=[0, 30, 50, 100],
                             labels=['Young', 'Middle-aged', 'Senior'])
    
    # Loan-to-income ratio
    df['loan_to_income'] = df['Amount'] / df['Income']
    
    # Interaction term: Rate * Percent_income
    df['rate_income_interaction'] = df['Rate'] * df['Percent_income']
    
    return df

# Apply feature engineering
X_engineered = create_features(X)

# Update categorical features to include new ones
cat_features += ['age_bucket']

# Check class imbalance
print("Class distribution:")
print(y.value_counts(normalize=True))

# We'll use SMOTE during model training to handle imbalance

# Stratified train-test split
X_train, X_test, y_train, y_test = train_test_split(
    X_engineered, y, test_size=0.2, random_state=42, stratify=y)

print(f"Training set shape: {X_train.shape}")
print(f"Test set shape: {X_test.shape}")
print("\nClass distribution in training set:")
print(y_train.value_counts(normalize=True))
print("\nClass distribution in test set:")
print(y_test.value_counts(normalize=True))

print(df.columns)

# Check for missing values in the dataset
missing_values = X_train.isnull().sum()

# Display the columns with missing values
print(missing_values[missing_values > 0])


# Clean columns
X = df.drop(columns=['Status', 'Id'])
y = df['Default']

# Updated preprocessing
numeric_features = ['Age', 'Income', 'Emp_length', 'Amount', 'Rate', 'Cred_length', 'Percent_income']
categorical_features = ['Home', 'Intent']

from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LogisticRegression
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import Pipeline as ImbPipeline
import joblib

# Preprocessing pipeline
numeric_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='median')),
    ('scaler', StandardScaler())
])
categorical_pipeline = Pipeline([
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('encoder', OneHotEncoder(handle_unknown='ignore'))
])
preprocessor = ColumnTransformer([
    ('num', numeric_pipeline, numeric_features),
    ('cat', categorical_pipeline, categorical_features)
])

# Final model pipeline
model_pipeline = ImbPipeline(steps=[
    ('preprocessor', preprocessor),
    ('smote', SMOTE(random_state=42)),
    ('classifier', LogisticRegression(max_iter=1000, random_state=42))
])

# Train
model_pipeline.fit(X_train, y_train)

# Save
joblib.dump(model_pipeline, 'best_credit_risk_model.pkl')
print("✅ Model retrained and saved.")

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer
from imblearn.over_sampling import SMOTE
from imblearn.pipeline import make_pipeline
import joblib

# Assume 'df' is your DataFrame

# Step 1: Feature Selection using Correlation
corr_matrix = df.corr(numeric_only=True)
threshold = 0.9
upper_tri = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))
high_corr_features = [column for column in upper_tri.columns if any(upper_tri[column].abs() > threshold)]
df_reduced = df.drop(columns=high_corr_features)

# Step 2: Separate features and target
X = df_reduced.drop(columns='Default')
y = df_reduced['Default']

# Step 3: Split data before any processing
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Step 4: Identify numerical and categorical columns
numeric_cols = X.select_dtypes(include=['int64', 'float64']).columns
categorical_cols = X.select_dtypes(include=['object', 'category']).columns

# Step 5: Create preprocessing pipelines
numeric_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='mean')),
    ('scaler', StandardScaler())
])

categorical_transformer = Pipeline(steps=[
    ('imputer', SimpleImputer(strategy='most_frequent')),
    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))
])

preprocessor = ColumnTransformer(
    transformers=[
        ('num', numeric_transformer, numeric_cols),
        ('cat', categorical_transformer, categorical_cols)
    ])

# Step 6: Create the full pipeline with SMOTE and model
pipeline = make_pipeline(
    preprocessor,
    SMOTE(random_state=42),
    RandomForestClassifier(random_state=42)
)

# Step 7: Train and evaluate
def evaluate_model(pipeline, X_train, y_train, X_test, y_test):
    pipeline.fit(X_train, y_train)
    
    y_pred = pipeline.predict(X_test)
    y_pred_proba = pipeline.predict_proba(X_test)[:, 1]

    print("Classification Report:")
    print(classification_report(y_test, y_pred))

    print("\nConfusion Matrix:")
    print(confusion_matrix(y_test, y_pred))

    roc_auc = roc_auc_score(y_test, y_pred_proba)
    print(f"\nROC-AUC Score: {roc_auc:.3f}")

    return pipeline

# Step 8: Train and evaluate Random Forest
print("Random Forest Results:")
rf_pipeline = evaluate_model(pipeline, X_train, y_train, X_test, y_test)

# Save the trained pipeline
joblib.dump(rf_pipeline, 'random_forest_pipeline.pkl')

from sklearn.ensemble import GradientBoostingClassifier

# Step 1: Create the Gradient Boosting pipeline
gb_pipeline = make_pipeline(
    preprocessor,  # Same preprocessor as before
    SMOTE(random_state=42),  # Same SMOTE configuration
    GradientBoostingClassifier(
        random_state=42,
        n_estimators=100,
        learning_rate=0.1,
        max_depth=3,
        min_samples_split=2,
        min_samples_leaf=1
    )
)

# Step 2: Train and evaluate Gradient Boosting
print("\nGradient Boosting Results:")
gb_model = evaluate_model(gb_pipeline, X_train, y_train, X_test, y_test)

# Step 3: Save the trained pipeline
joblib.dump(gb_model, 'gradient_boosting_pipeline.pkl')

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score
from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, roc_auc_score
import pandas as pd

def plot_model_performance(pipelines, model_names, X_test_raw, y_test):
    plt.figure(figsize=(15, 6))
    
    # ROC Curve
    plt.subplot(1, 2, 1)
    for i, pipeline in enumerate(pipelines):
        # Use predict_proba on the raw data (let pipeline handle preprocessing)
        y_pred_proba = pipeline.predict_proba(X_test_raw)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_pred_proba)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, lw=2, label=f'{model_names[i]} (AUC = {roc_auc:.2f})')
    
    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
    plt.xlim([0.0, 1.0])
    plt.ylim([0.0, 1.05])
    plt.xlabel('False Positive Rate')
    plt.ylabel('True Positive Rate')
    plt.title('Receiver Operating Characteristic (ROC) Curve')
    plt.legend(loc="lower right")
    
    # Precision-Recall Curve
    plt.subplot(1, 2, 2)
    for i, pipeline in enumerate(pipelines):
        y_pred_proba = pipeline.predict_proba(X_test_raw)[:, 1]
        precision, recall, _ = precision_recall_curve(y_test, y_pred_proba)
        avg_precision = average_precision_score(y_test, y_pred_proba)
        plt.plot(recall, precision, lw=2, 
                label=f'{model_names[i]} (AP = {avg_precision:.2f})')
    
    plt.xlabel('Recall')
    plt.ylabel('Precision')
    plt.ylim([0.0, 1.05])
    plt.xlim([0.0, 1.0])
    plt.title('Precision-Recall Curve')
    plt.legend(loc="lower left")
    
    plt.tight_layout()
    plt.show()

    # Metrics Table
    metrics = []
    for pipeline, name in zip(pipelines, model_names):
        y_pred = pipeline.predict(X_test_raw)
        y_pred_proba = pipeline.predict_proba(X_test_raw)[:, 1]
        
        metrics.append({
            'Model': name,
            'Accuracy': accuracy_score(y_test, y_pred),
            'Precision': precision_score(y_test, y_pred),
            'Recall': recall_score(y_test, y_pred),
            'F1 Score': f1_score(y_test, y_pred),
            'ROC AUC': roc_auc_score(y_test, y_pred_proba),
            'PR AUC': average_precision_score(y_test, y_pred_proba)
        })

    metrics_df = pd.DataFrame(metrics)
    print("\nPerformance Metrics Comparison:")
    print(metrics_df.to_markdown(index=False))
    return metrics_df

# Usage example:
# trained_pipelines = [lr_pipeline, rf_pipeline, gb_pipeline]  # Your complete pipelines
# model_names = ['Logistic Regression', 'Random Forest', 'Gradient Boosting']
# metrics_df = plot_model_performance(trained_pipelines, model_names, X_test, y_test)